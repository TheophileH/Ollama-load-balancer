services:
  # --- Load Balancer (Rust Version) ---
  load-balancer:
    build:
      context: ${LB_BUILD_CONTEXT:-./Rust_lb}
      dockerfile: ${LB_DOCKERFILE:-Dockerfile}
    container_name: ${OLLAMA_LB_CONTAINER_NAME:-load-balancer}
    pid: "host" # Required for nsenter
    privileged: true # Required for nsenter
    ports:
      - "${PORT:-11434}:11434"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      # Map generic LOG_LEVEL to Rust's specific env var
      - RUST_LOG=${LOG_LEVEL:-info}
      # Also pass LOG_LEVEL for Python fallback
      - LOG_LEVEL=${LOG_LEVEL:-info}
    env_file: .env

    deploy:
      resources:
        limits:
          memory: 128M # Rust is tiny
    restart: always
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:11434/api/version" ]
      interval: 30s
      timeout: 10s
      retries: 3

  # --- NVIDIA Backend Container 1 (Primary GPU 0) ---
  ollama-nvidia-1:
    image: ${OLLAMA_NVIDIA_IMAGE:-ollama/ollama:latest}
    container_name: ollama-nvidia-1
    runtime: nvidia
    labels:
      - "ollama.backend=true"
      - "ollama.type=nvidia"
    ports:
      - "11434"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=${NVIDIA_GPU_1:-0,1,2}
      - OLLAMA_HOST=${OLLAMA_HOST:-0.0.0.0:11434}
      - OLLAMA_ORIGINS=${OLLAMA_ORIGINS:-*}
      - OLLAMA_MODELS=/root/.ollama/models
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-4}
      - OLLAMA_FLASH_ATTENTION=${OLLAMA_FLASH_ATTENTION:-1}
      - OLLAMA_KV_CACHE_TYPE=${OLLAMA_KV_CACHE_TYPE:-f16}
      - OLLAMA_CONTEXT_LENGTH=${OLLAMA_CONTEXT_LENGTH:-2048}
    volumes:
      - ollama_storage:/root/.ollama
    restart: always

  # --- NVIDIA Backend Container 2 (Primary GPU 1) ---
  ollama-nvidia-2:
    image: ${OLLAMA_NVIDIA_IMAGE:-ollama/ollama:latest}
    container_name: ollama-nvidia-2
    runtime: nvidia
    labels:
      - "ollama.backend=true"
      - "ollama.type=nvidia"
    ports:
      - "11434"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=${NVIDIA_GPU_2:-1,2,0}
      - OLLAMA_HOST=${OLLAMA_HOST:-0.0.0.0:11434}
      - OLLAMA_ORIGINS=${OLLAMA_ORIGINS:-*}
      - OLLAMA_MODELS=/root/.ollama/models
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-4}
      - OLLAMA_FLASH_ATTENTION=${OLLAMA_FLASH_ATTENTION:-1}
      - OLLAMA_KV_CACHE_TYPE=${OLLAMA_KV_CACHE_TYPE:-f16}
      - OLLAMA_CONTEXT_LENGTH=${OLLAMA_CONTEXT_LENGTH:-2048}
    volumes:
      - ollama_storage:/root/.ollama
    restart: always

  # --- NVIDIA Backend Container 3 (Primary GPU 2) ---
  ollama-nvidia-3:
    image: ${OLLAMA_NVIDIA_IMAGE:-ollama/ollama:latest}
    container_name: ollama-nvidia-3
    runtime: nvidia
    labels:
      - "ollama.backend=true"
      - "ollama.type=nvidia"
    ports:
      - "11434"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=${NVIDIA_GPU_3:-2,0,1}
      - OLLAMA_HOST=${OLLAMA_HOST:-0.0.0.0:11434}
      - OLLAMA_ORIGINS=${OLLAMA_ORIGINS:-*}
      - OLLAMA_MODELS=/root/.ollama/models
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-4}
      - OLLAMA_FLASH_ATTENTION=${OLLAMA_FLASH_ATTENTION:-1}
      - OLLAMA_KV_CACHE_TYPE=${OLLAMA_KV_CACHE_TYPE:-f16}
      - OLLAMA_CONTEXT_LENGTH=${OLLAMA_CONTEXT_LENGTH:-2048}
    volumes:
      - ollama_storage:/root/.ollama
    restart: always

  # --- AMD Backend Container 1 (Primary GPU 0) ---
  ollama-amd-1:
    image: ${OLLAMA_AMD_IMAGE:-ollama/ollama:rocm}
    container_name: ollama-amd-1
    labels:
      - "ollama.backend=true"
      - "ollama.type=amd"
    ports:
      - "11434"
    devices:
      - /dev/kfd
      - /dev/dri
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST:-0.0.0.0:11434}
      - OLLAMA_ORIGINS=${OLLAMA_ORIGINS:-*}
      - OLLAMA_MODELS=/root/.ollama/models
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-4}
      # AMD specific envs
      - HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION:-10.3.0}
      - HCC_AMDGPU_TARGET=${HCC_AMDGPU_TARGET:-gfx1030}
      - ROCR_VISIBLE_DEVICES=${AMD_GPU_1:-0,1}
      - HIP_VISIBLE_DEVICES=${AMD_GPU_1:-0,1}
      - OLLAMA_CONTEXT_LENGTH=${OLLAMA_CONTEXT_LENGTH:-2048}
      - OLLAMA_DEBUG=${OLLAMA_DEBUG}
    group_add:
      - "44"
      - "109"
    security_opt:
      - seccomp:unconfined
    volumes:
      - ollama_storage:/root/.ollama
    restart: always

volumes:
  ollama_storage:
    name: ${OLLAMA_DATA_VOLUME:-ollama}

